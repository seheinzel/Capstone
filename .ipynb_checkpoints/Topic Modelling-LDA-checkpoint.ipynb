{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, Ridge\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim.corpora as corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"combined_season1-37.tsv.zip\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round</th>\n",
       "      <th>value</th>\n",
       "      <th>daily_double</th>\n",
       "      <th>category</th>\n",
       "      <th>comments</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>air_date</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>no</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>River mentioned most often in the Bible</td>\n",
       "      <td>the Jordan</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>no</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>Scottish word for lake</td>\n",
       "      <td>loch</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>yes</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>River in this famous song:</td>\n",
       "      <td>the Volga River</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>no</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>American river only 33 miles shorter than the ...</td>\n",
       "      <td>the Missouri</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>no</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>World's largest lake, nearly 5 times as big as...</td>\n",
       "      <td>the Caspian Sea</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389440</th>\n",
       "      <td>2</td>\n",
       "      <td>400</td>\n",
       "      <td>no</td>\n",
       "      <td>FOUNDRY</td>\n",
       "      <td>-</td>\n",
       "      <td>This hefty noisemaker from Whitechapel Foundry...</td>\n",
       "      <td>Big Ben</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389441</th>\n",
       "      <td>2</td>\n",
       "      <td>800</td>\n",
       "      <td>no</td>\n",
       "      <td>FOUNDRY</td>\n",
       "      <td>-</td>\n",
       "      <td>Around 4,000 years ago, the first foundries in...</td>\n",
       "      <td>bronze</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389442</th>\n",
       "      <td>2</td>\n",
       "      <td>1200</td>\n",
       "      <td>no</td>\n",
       "      <td>FOUNDRY</td>\n",
       "      <td>-</td>\n",
       "      <td>Several different foundries worked for 4 month...</td>\n",
       "      <td>Monitor</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389443</th>\n",
       "      <td>2</td>\n",
       "      <td>1600</td>\n",
       "      <td>no</td>\n",
       "      <td>FOUNDRY</td>\n",
       "      <td>-</td>\n",
       "      <td>Once one of the largest of its kind, the Gary ...</td>\n",
       "      <td>U.S. Steel</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389444</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>19th CENTURY AMERICAN WOMEN</td>\n",
       "      <td>-</td>\n",
       "      <td>2 of the 3 women depicted on the first statue ...</td>\n",
       "      <td>(2 of) (Sojourner) Truth, (Susan B.) Anthony, ...</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>389445 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        round  value daily_double                     category comments  \\\n",
       "0           1    100           no               LAKES & RIVERS        -   \n",
       "1           1    200           no               LAKES & RIVERS        -   \n",
       "2           1    800          yes               LAKES & RIVERS        -   \n",
       "3           1    400           no               LAKES & RIVERS        -   \n",
       "4           1    500           no               LAKES & RIVERS        -   \n",
       "...       ...    ...          ...                          ...      ...   \n",
       "389440      2    400           no                      FOUNDRY        -   \n",
       "389441      2    800           no                      FOUNDRY        -   \n",
       "389442      2   1200           no                      FOUNDRY        -   \n",
       "389443      2   1600           no                      FOUNDRY        -   \n",
       "389444      3      0           no  19th CENTURY AMERICAN WOMEN        -   \n",
       "\n",
       "                                                   answer  \\\n",
       "0                 River mentioned most often in the Bible   \n",
       "1                                  Scottish word for lake   \n",
       "2                              River in this famous song:   \n",
       "3       American river only 33 miles shorter than the ...   \n",
       "4       World's largest lake, nearly 5 times as big as...   \n",
       "...                                                   ...   \n",
       "389440  This hefty noisemaker from Whitechapel Foundry...   \n",
       "389441  Around 4,000 years ago, the first foundries in...   \n",
       "389442  Several different foundries worked for 4 month...   \n",
       "389443  Once one of the largest of its kind, the Gary ...   \n",
       "389444  2 of the 3 women depicted on the first statue ...   \n",
       "\n",
       "                                                 question    air_date notes  \n",
       "0                                              the Jordan  1984-09-10     -  \n",
       "1                                                    loch  1984-09-10     -  \n",
       "2                                         the Volga River  1984-09-10     -  \n",
       "3                                            the Missouri  1984-09-10     -  \n",
       "4                                         the Caspian Sea  1984-09-10     -  \n",
       "...                                                   ...         ...   ...  \n",
       "389440                                            Big Ben  2021-08-13     -  \n",
       "389441                                             bronze  2021-08-13     -  \n",
       "389442                                            Monitor  2021-08-13     -  \n",
       "389443                                         U.S. Steel  2021-08-13     -  \n",
       "389444  (2 of) (Sojourner) Truth, (Susan B.) Anthony, ...  2021-08-13     -  \n",
       "\n",
       "[389445 rows x 9 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['air_date'] = pd.to_datetime(df['air_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On 11/26/2001, the values for the questions doubled for both rounds of Jeopardy. Need to adjust the earlier episodes to have the same values as post-11/26/2001 shows.\n",
    "df.loc[df['air_date'] < '2001-11-26', \"value\"] = df.value * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove Daily Doubles since the contestants can wager any amounts for those\n",
    "df = df[df[\"daily_double\"] != 'yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only standard values (this will remove Final Jeopardy questions, which do not have a set amount and set are at '0', as well as the handful of non-standard values that are likely typos)\n",
    "df = df.loc[df['value'].isin([200, 400, 600, 800, 1000, 400, 800, 1200, 1600, 2000])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column that contains questions, answers, and category\n",
    "df[\"q_a_and_cat\"] = df[\"answer\"] + ' ' + df[\"question\"] + ' ' + df[\"category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round</th>\n",
       "      <th>value</th>\n",
       "      <th>daily_double</th>\n",
       "      <th>category</th>\n",
       "      <th>comments</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>air_date</th>\n",
       "      <th>notes</th>\n",
       "      <th>q_and_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>no</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>River mentioned most often in the Bible</td>\n",
       "      <td>the Jordan</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "      <td>[River, mentioned, most, often, in, the, Bible...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>no</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>Scottish word for lake</td>\n",
       "      <td>loch</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "      <td>[Scottish, word, for, lake, loch]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>no</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>American river only 33 miles shorter than the ...</td>\n",
       "      <td>the Missouri</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "      <td>[American, river, only, 33, mile, shorter, tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>no</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>World's largest lake, nearly 5 times as big as...</td>\n",
       "      <td>the Caspian Sea</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "      <td>[World, 's, largest, lake, ,, nearly, 5, time,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>no</td>\n",
       "      <td>INVENTIONS</td>\n",
       "      <td>-</td>\n",
       "      <td>Marconi's wonderful wireless</td>\n",
       "      <td>the radio</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "      <td>[Marconi, 's, wonderful, wireless, the, radio]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389438</th>\n",
       "      <td>2</td>\n",
       "      <td>1600</td>\n",
       "      <td>no</td>\n",
       "      <td>LOST</td>\n",
       "      <td>-</td>\n",
       "      <td>In \"A Moveable Feast\", Gertrude Stein is quote...</td>\n",
       "      <td>Lost Generation</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "      <td>[In, ``, A, Moveable, Feast, '', ,, Gertrude, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389440</th>\n",
       "      <td>2</td>\n",
       "      <td>400</td>\n",
       "      <td>no</td>\n",
       "      <td>FOUNDRY</td>\n",
       "      <td>-</td>\n",
       "      <td>This hefty noisemaker from Whitechapel Foundry...</td>\n",
       "      <td>Big Ben</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "      <td>[This, hefty, noisemaker, from, Whitechapel, F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389441</th>\n",
       "      <td>2</td>\n",
       "      <td>800</td>\n",
       "      <td>no</td>\n",
       "      <td>FOUNDRY</td>\n",
       "      <td>-</td>\n",
       "      <td>Around 4,000 years ago, the first foundries in...</td>\n",
       "      <td>bronze</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "      <td>[Around, 4,000, year, ago, ,, the, first, foun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389442</th>\n",
       "      <td>2</td>\n",
       "      <td>1200</td>\n",
       "      <td>no</td>\n",
       "      <td>FOUNDRY</td>\n",
       "      <td>-</td>\n",
       "      <td>Several different foundries worked for 4 month...</td>\n",
       "      <td>Monitor</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "      <td>[Several, different, foundry, worked, for, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389443</th>\n",
       "      <td>2</td>\n",
       "      <td>1600</td>\n",
       "      <td>no</td>\n",
       "      <td>FOUNDRY</td>\n",
       "      <td>-</td>\n",
       "      <td>Once one of the largest of its kind, the Gary ...</td>\n",
       "      <td>U.S. Steel</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "      <td>[Once, one, of, the, largest, of, it, kind, ,,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>363765 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        round  value daily_double        category comments  \\\n",
       "0           1    200           no  LAKES & RIVERS        -   \n",
       "1           1    400           no  LAKES & RIVERS        -   \n",
       "3           1    800           no  LAKES & RIVERS        -   \n",
       "4           1   1000           no  LAKES & RIVERS        -   \n",
       "5           1    200           no      INVENTIONS        -   \n",
       "...       ...    ...          ...             ...      ...   \n",
       "389438      2   1600           no            LOST        -   \n",
       "389440      2    400           no         FOUNDRY        -   \n",
       "389441      2    800           no         FOUNDRY        -   \n",
       "389442      2   1200           no         FOUNDRY        -   \n",
       "389443      2   1600           no         FOUNDRY        -   \n",
       "\n",
       "                                                   answer         question  \\\n",
       "0                 River mentioned most often in the Bible       the Jordan   \n",
       "1                                  Scottish word for lake             loch   \n",
       "3       American river only 33 miles shorter than the ...     the Missouri   \n",
       "4       World's largest lake, nearly 5 times as big as...  the Caspian Sea   \n",
       "5                            Marconi's wonderful wireless        the radio   \n",
       "...                                                   ...              ...   \n",
       "389438  In \"A Moveable Feast\", Gertrude Stein is quote...  Lost Generation   \n",
       "389440  This hefty noisemaker from Whitechapel Foundry...          Big Ben   \n",
       "389441  Around 4,000 years ago, the first foundries in...           bronze   \n",
       "389442  Several different foundries worked for 4 month...          Monitor   \n",
       "389443  Once one of the largest of its kind, the Gary ...       U.S. Steel   \n",
       "\n",
       "         air_date notes                                            q_and_a  \n",
       "0      1984-09-10     -  [River, mentioned, most, often, in, the, Bible...  \n",
       "1      1984-09-10     -                  [Scottish, word, for, lake, loch]  \n",
       "3      1984-09-10     -  [American, river, only, 33, mile, shorter, tha...  \n",
       "4      1984-09-10     -  [World, 's, largest, lake, ,, nearly, 5, time,...  \n",
       "5      1984-09-10     -     [Marconi, 's, wonderful, wireless, the, radio]  \n",
       "...           ...   ...                                                ...  \n",
       "389438 2021-08-13     -  [In, ``, A, Moveable, Feast, '', ,, Gertrude, ...  \n",
       "389440 2021-08-13     -  [This, hefty, noisemaker, from, Whitechapel, F...  \n",
       "389441 2021-08-13     -  [Around, 4,000, year, ago, ,, the, first, foun...  \n",
       "389442 2021-08-13     -  [Several, different, foundry, worked, for, 4, ...  \n",
       "389443 2021-08-13     -  [Once, one, of, the, largest, of, it, kind, ,,...  \n",
       "\n",
       "[363765 rows x 10 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenize words in \"q_and_a\" column\n",
    "df['q_and_a'] = df['q_and_a'].apply(word_tokenize)\n",
    "\n",
    "#lemmatize \"q_and_a\" column\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['q_and_a'] = df['q_and_a'].apply(lambda row:[lemmatizer.lemmatize(word) for word in row])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round</th>\n",
       "      <th>value</th>\n",
       "      <th>daily_double</th>\n",
       "      <th>category</th>\n",
       "      <th>comments</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>air_date</th>\n",
       "      <th>notes</th>\n",
       "      <th>q_and_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>no</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>River mentioned most often in the Bible</td>\n",
       "      <td>the Jordan</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "      <td>R i v e r   m e n t i o n e d   m o s t   o f ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>no</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>Scottish word for lake</td>\n",
       "      <td>loch</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "      <td>S c o t t i s h   w o r d   f o r   l a k e   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>no</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>American river only 33 miles shorter than the ...</td>\n",
       "      <td>the Missouri</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "      <td>A m e r i c a n   r i v e r   o n l y   3 3   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>no</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>World's largest lake, nearly 5 times as big as...</td>\n",
       "      <td>the Caspian Sea</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "      <td>W o r l d ' s   l a r g e s t   l a k e ,   n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>no</td>\n",
       "      <td>INVENTIONS</td>\n",
       "      <td>-</td>\n",
       "      <td>Marconi's wonderful wireless</td>\n",
       "      <td>the radio</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "      <td>M a r c o n i ' s   w o n d e r f u l   w i r ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389438</th>\n",
       "      <td>2</td>\n",
       "      <td>1600</td>\n",
       "      <td>no</td>\n",
       "      <td>LOST</td>\n",
       "      <td>-</td>\n",
       "      <td>In \"A Moveable Feast\", Gertrude Stein is quote...</td>\n",
       "      <td>Lost Generation</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "      <td>I n   \" A   M o v e a b l e   F e a s t \" ,   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389440</th>\n",
       "      <td>2</td>\n",
       "      <td>400</td>\n",
       "      <td>no</td>\n",
       "      <td>FOUNDRY</td>\n",
       "      <td>-</td>\n",
       "      <td>This hefty noisemaker from Whitechapel Foundry...</td>\n",
       "      <td>Big Ben</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "      <td>T h i s   h e f t y   n o i s e m a k e r   f ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389441</th>\n",
       "      <td>2</td>\n",
       "      <td>800</td>\n",
       "      <td>no</td>\n",
       "      <td>FOUNDRY</td>\n",
       "      <td>-</td>\n",
       "      <td>Around 4,000 years ago, the first foundries in...</td>\n",
       "      <td>bronze</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "      <td>A r o u n d   4 , 0 0 0   y e a r s   a g o , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389442</th>\n",
       "      <td>2</td>\n",
       "      <td>1200</td>\n",
       "      <td>no</td>\n",
       "      <td>FOUNDRY</td>\n",
       "      <td>-</td>\n",
       "      <td>Several different foundries worked for 4 month...</td>\n",
       "      <td>Monitor</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "      <td>S e v e r a l   d i f f e r e n t   f o u n d ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389443</th>\n",
       "      <td>2</td>\n",
       "      <td>1600</td>\n",
       "      <td>no</td>\n",
       "      <td>FOUNDRY</td>\n",
       "      <td>-</td>\n",
       "      <td>Once one of the largest of its kind, the Gary ...</td>\n",
       "      <td>U.S. Steel</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "      <td>O n c e   o n e   o f   t h e   l a r g e s t ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>363765 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        round  value daily_double        category comments  \\\n",
       "0           1    200           no  LAKES & RIVERS        -   \n",
       "1           1    400           no  LAKES & RIVERS        -   \n",
       "3           1    800           no  LAKES & RIVERS        -   \n",
       "4           1   1000           no  LAKES & RIVERS        -   \n",
       "5           1    200           no      INVENTIONS        -   \n",
       "...       ...    ...          ...             ...      ...   \n",
       "389438      2   1600           no            LOST        -   \n",
       "389440      2    400           no         FOUNDRY        -   \n",
       "389441      2    800           no         FOUNDRY        -   \n",
       "389442      2   1200           no         FOUNDRY        -   \n",
       "389443      2   1600           no         FOUNDRY        -   \n",
       "\n",
       "                                                   answer         question  \\\n",
       "0                 River mentioned most often in the Bible       the Jordan   \n",
       "1                                  Scottish word for lake             loch   \n",
       "3       American river only 33 miles shorter than the ...     the Missouri   \n",
       "4       World's largest lake, nearly 5 times as big as...  the Caspian Sea   \n",
       "5                            Marconi's wonderful wireless        the radio   \n",
       "...                                                   ...              ...   \n",
       "389438  In \"A Moveable Feast\", Gertrude Stein is quote...  Lost Generation   \n",
       "389440  This hefty noisemaker from Whitechapel Foundry...          Big Ben   \n",
       "389441  Around 4,000 years ago, the first foundries in...           bronze   \n",
       "389442  Several different foundries worked for 4 month...          Monitor   \n",
       "389443  Once one of the largest of its kind, the Gary ...       U.S. Steel   \n",
       "\n",
       "         air_date notes                                            q_and_a  \n",
       "0      1984-09-10     -  R i v e r   m e n t i o n e d   m o s t   o f ...  \n",
       "1      1984-09-10     -  S c o t t i s h   w o r d   f o r   l a k e   ...  \n",
       "3      1984-09-10     -  A m e r i c a n   r i v e r   o n l y   3 3   ...  \n",
       "4      1984-09-10     -  W o r l d ' s   l a r g e s t   l a k e ,   n ...  \n",
       "5      1984-09-10     -  M a r c o n i ' s   w o n d e r f u l   w i r ...  \n",
       "...           ...   ...                                                ...  \n",
       "389438 2021-08-13     -  I n   \" A   M o v e a b l e   F e a s t \" ,   ...  \n",
       "389440 2021-08-13     -  T h i s   h e f t y   n o i s e m a k e r   f ...  \n",
       "389441 2021-08-13     -  A r o u n d   4 , 0 0 0   y e a r s   a g o , ...  \n",
       "389442 2021-08-13     -  S e v e r a l   d i f f e r e n t   f o u n d ...  \n",
       "389443 2021-08-13     -  O n c e   o n e   o f   t h e   l a r g e s t ...  \n",
       "\n",
       "[363765 rows x 10 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change the column so it's not a list of lists\n",
    "df['q_and_a'] = df['q_and_a'].str.join(\" \")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round</th>\n",
       "      <th>value</th>\n",
       "      <th>daily_double</th>\n",
       "      <th>category</th>\n",
       "      <th>comments</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>air_date</th>\n",
       "      <th>notes</th>\n",
       "      <th>q_and_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>no</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>River mentioned most often in the Bible</td>\n",
       "      <td>the Jordan</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "      <td>River mentioned often Bible Jordan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>no</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>Scottish word for lake</td>\n",
       "      <td>loch</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "      <td>Scottish word lake loch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>no</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>American river only 33 miles shorter than the ...</td>\n",
       "      <td>the Missouri</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "      <td>American river 33 mile shorter Mississippi Mis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>no</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>World's largest lake, nearly 5 times as big as...</td>\n",
       "      <td>the Caspian Sea</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "      <td>World 's largest lake , nearly 5 time big Supe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>no</td>\n",
       "      <td>INVENTIONS</td>\n",
       "      <td>-</td>\n",
       "      <td>Marconi's wonderful wireless</td>\n",
       "      <td>the radio</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "      <td>Marconi 's wonderful wireless radio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389438</th>\n",
       "      <td>2</td>\n",
       "      <td>1600</td>\n",
       "      <td>no</td>\n",
       "      <td>LOST</td>\n",
       "      <td>-</td>\n",
       "      <td>In \"A Moveable Feast\", Gertrude Stein is quote...</td>\n",
       "      <td>Lost Generation</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "      <td>In `` A Moveable Feast '' , Gertrude Stein quo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389440</th>\n",
       "      <td>2</td>\n",
       "      <td>400</td>\n",
       "      <td>no</td>\n",
       "      <td>FOUNDRY</td>\n",
       "      <td>-</td>\n",
       "      <td>This hefty noisemaker from Whitechapel Foundry...</td>\n",
       "      <td>Big Ben</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "      <td>This hefty noisemaker Whitechapel Foundry bega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389441</th>\n",
       "      <td>2</td>\n",
       "      <td>800</td>\n",
       "      <td>no</td>\n",
       "      <td>FOUNDRY</td>\n",
       "      <td>-</td>\n",
       "      <td>Around 4,000 years ago, the first foundries in...</td>\n",
       "      <td>bronze</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "      <td>Around 4,000 year ago , first foundry Mesopota...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389442</th>\n",
       "      <td>2</td>\n",
       "      <td>1200</td>\n",
       "      <td>no</td>\n",
       "      <td>FOUNDRY</td>\n",
       "      <td>-</td>\n",
       "      <td>Several different foundries worked for 4 month...</td>\n",
       "      <td>Monitor</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "      <td>Several different foundry worked 4 month build...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389443</th>\n",
       "      <td>2</td>\n",
       "      <td>1600</td>\n",
       "      <td>no</td>\n",
       "      <td>FOUNDRY</td>\n",
       "      <td>-</td>\n",
       "      <td>Once one of the largest of its kind, the Gary ...</td>\n",
       "      <td>U.S. Steel</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "      <td>Once one largest kind , Gary Works Indiana sti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>363765 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        round  value daily_double        category comments  \\\n",
       "0           1    200           no  LAKES & RIVERS        -   \n",
       "1           1    400           no  LAKES & RIVERS        -   \n",
       "3           1    800           no  LAKES & RIVERS        -   \n",
       "4           1   1000           no  LAKES & RIVERS        -   \n",
       "5           1    200           no      INVENTIONS        -   \n",
       "...       ...    ...          ...             ...      ...   \n",
       "389438      2   1600           no            LOST        -   \n",
       "389440      2    400           no         FOUNDRY        -   \n",
       "389441      2    800           no         FOUNDRY        -   \n",
       "389442      2   1200           no         FOUNDRY        -   \n",
       "389443      2   1600           no         FOUNDRY        -   \n",
       "\n",
       "                                                   answer         question  \\\n",
       "0                 River mentioned most often in the Bible       the Jordan   \n",
       "1                                  Scottish word for lake             loch   \n",
       "3       American river only 33 miles shorter than the ...     the Missouri   \n",
       "4       World's largest lake, nearly 5 times as big as...  the Caspian Sea   \n",
       "5                            Marconi's wonderful wireless        the radio   \n",
       "...                                                   ...              ...   \n",
       "389438  In \"A Moveable Feast\", Gertrude Stein is quote...  Lost Generation   \n",
       "389440  This hefty noisemaker from Whitechapel Foundry...          Big Ben   \n",
       "389441  Around 4,000 years ago, the first foundries in...           bronze   \n",
       "389442  Several different foundries worked for 4 month...          Monitor   \n",
       "389443  Once one of the largest of its kind, the Gary ...       U.S. Steel   \n",
       "\n",
       "         air_date notes                                            q_and_a  \n",
       "0      1984-09-10     -                 River mentioned often Bible Jordan  \n",
       "1      1984-09-10     -                            Scottish word lake loch  \n",
       "3      1984-09-10     -  American river 33 mile shorter Mississippi Mis...  \n",
       "4      1984-09-10     -  World 's largest lake , nearly 5 time big Supe...  \n",
       "5      1984-09-10     -                Marconi 's wonderful wireless radio  \n",
       "...           ...   ...                                                ...  \n",
       "389438 2021-08-13     -  In `` A Moveable Feast '' , Gertrude Stein quo...  \n",
       "389440 2021-08-13     -  This hefty noisemaker Whitechapel Foundry bega...  \n",
       "389441 2021-08-13     -  Around 4,000 year ago , first foundry Mesopota...  \n",
       "389442 2021-08-13     -  Several different foundry worked 4 month build...  \n",
       "389443 2021-08-13     -  Once one largest kind , Gary Works Indiana sti...  \n",
       "\n",
       "[363765 rows x 10 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove stopwords\n",
    "stop = stopwords.words('english')\n",
    "df['q_and_a'] = df['q_and_a'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import string to remove punctuation\n",
    "\n",
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_punctuation(text):\n",
    "    nopunct=[words for words in text if words not in string.punctuation]\n",
    "    words_without_punct=''.join(nopunct)\n",
    "    return words_without_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation and lowercase words in 'category,' 'answer,' and 'question'\n",
    "\n",
    "df['q_and_a'] = df['q_and_a'].apply(lambda x: no_punctuation(x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove numerals \n",
    "\n",
    "df['q_and_a'] = df['q_and_a'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-00cf07b74dcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use CountVectorizer to make a bag of words matrix, excluding stopwords, triwords\n",
    "vectorizer = CountVectorizer(max_features=1000, stop_words='english', ngram_range= (1, 3))\n",
    "vectorized_answer = vectorizer.fit_transform(df.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363765, 1000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_answer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03333333, 0.03333333, 0.03333333, ..., 0.36666665, 0.03333333,\n",
       "        0.36666668],\n",
       "       [0.025     , 0.025     , 0.525     , ..., 0.025     , 0.025     ,\n",
       "        0.025     ],\n",
       "       [0.025     , 0.025     , 0.025     , ..., 0.025     , 0.27499606,\n",
       "        0.52499812],\n",
       "       ...,\n",
       "       [0.03333333, 0.03333333, 0.03333333, ..., 0.03333333, 0.36666666,\n",
       "        0.03333333],\n",
       "       [0.27499999, 0.025     , 0.025     , ..., 0.025     , 0.025     ,\n",
       "        0.025     ],\n",
       "       [0.025     , 0.025     , 0.025     , ..., 0.025     , 0.025     ,\n",
       "        0.52499765]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run LDA\n",
    "lda = LatentDirichletAllocation(n_components=10, learning_method='online', random_state=54)\n",
    "lda.fit_transform(vectorized_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))                        \n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "said letter people play hit dont way family know shows short death god head wife reports led girl leader blue law want includes takes refers thomas told citys books killed baby paul tom central animals hero drama money lord european\n",
      "Topic #1:\n",
      "new th war make life century york body took gave team including battle german father countrys general robert kids court military look things april real ship june run female bad daughter royal mans delivers square foot prince united civil yearold\n",
      "Topic #2:\n",
      "word clue country book song says company little use red musical number near right large help color car light late ones women building david heart win flag dance region piece does san mary sun western doesnt winner county bridge governor\n",
      "Topic #3:\n",
      "called crew years time king won im meaning latin set north museum founded story high feet university second night youll college art dog african oscar bc begins held event stands charles species going original islands field future fruit uses given\n",
      "Topic #4:\n",
      "named known got british character work star white died based created hes rock america bird age left history away england green air phrase didnt hall live actress published ii boy role style dr tale really common symbol married better tells\n",
      "Topic #5:\n",
      "type used just played french home big national game island small say times english son person george lake ancient brand men band guy william gets worlds main nickname helped washington mr street lady fish henry space southern texas native brother\n",
      "Topic #6:\n",
      "city like state world capital great group thats long school black area town nation major food spanish version adjective language island july act tree poem official inspired order originally sound sounds russian africa types hand written jack public free empire\n",
      "Topic #7:\n",
      "means president john st woman good famous water sarah sea jimmy later came popular land come kelly center gives served did party earth site today france prize power army middle included goes plays seen dead theyre office half michael room\n",
      "Topic #8:\n",
      "man film novel term wrote day american year youre tv born place comes author include million began park end states west classic went presents early actor using queen best words makes built days california point making gold march introduced east\n",
      "Topic #9:\n",
      "title river south old house movie series love greek line james music young italian largest form miles la monitor animal mean singer lost kind job mountain playing opera comedy member theres record sport mother government instrument think church horse giant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "print_top_words(lda, tf_feature_names, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round</th>\n",
       "      <th>value</th>\n",
       "      <th>daily_double</th>\n",
       "      <th>category</th>\n",
       "      <th>comments</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>air_date</th>\n",
       "      <th>notes</th>\n",
       "      <th>q_a_and_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>no</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>River mentioned most often in the Bible</td>\n",
       "      <td>the Jordan</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "      <td>River mentioned most often in the Bible the Jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>no</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>Scottish word for lake</td>\n",
       "      <td>loch</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "      <td>Scottish word for lake loch LAKES &amp; RIVERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>no</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>American river only 33 miles shorter than the ...</td>\n",
       "      <td>the Missouri</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "      <td>American river only 33 miles shorter than the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>no</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>World's largest lake, nearly 5 times as big as...</td>\n",
       "      <td>the Caspian Sea</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "      <td>World's largest lake, nearly 5 times as big as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>no</td>\n",
       "      <td>INVENTIONS</td>\n",
       "      <td>-</td>\n",
       "      <td>Marconi's wonderful wireless</td>\n",
       "      <td>the radio</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "      <td>Marconi's wonderful wireless the radio INVENTIONS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389438</th>\n",
       "      <td>2</td>\n",
       "      <td>1600</td>\n",
       "      <td>no</td>\n",
       "      <td>LOST</td>\n",
       "      <td>-</td>\n",
       "      <td>In \"A Moveable Feast\", Gertrude Stein is quote...</td>\n",
       "      <td>Lost Generation</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "      <td>In \"A Moveable Feast\", Gertrude Stein is quote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389440</th>\n",
       "      <td>2</td>\n",
       "      <td>400</td>\n",
       "      <td>no</td>\n",
       "      <td>FOUNDRY</td>\n",
       "      <td>-</td>\n",
       "      <td>This hefty noisemaker from Whitechapel Foundry...</td>\n",
       "      <td>Big Ben</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "      <td>This hefty noisemaker from Whitechapel Foundry...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389441</th>\n",
       "      <td>2</td>\n",
       "      <td>800</td>\n",
       "      <td>no</td>\n",
       "      <td>FOUNDRY</td>\n",
       "      <td>-</td>\n",
       "      <td>Around 4,000 years ago, the first foundries in...</td>\n",
       "      <td>bronze</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "      <td>Around 4,000 years ago, the first foundries in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389442</th>\n",
       "      <td>2</td>\n",
       "      <td>1200</td>\n",
       "      <td>no</td>\n",
       "      <td>FOUNDRY</td>\n",
       "      <td>-</td>\n",
       "      <td>Several different foundries worked for 4 month...</td>\n",
       "      <td>Monitor</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "      <td>Several different foundries worked for 4 month...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389443</th>\n",
       "      <td>2</td>\n",
       "      <td>1600</td>\n",
       "      <td>no</td>\n",
       "      <td>FOUNDRY</td>\n",
       "      <td>-</td>\n",
       "      <td>Once one of the largest of its kind, the Gary ...</td>\n",
       "      <td>U.S. Steel</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "      <td>Once one of the largest of its kind, the Gary ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>363765 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        round  value daily_double        category comments  \\\n",
       "0           1    200           no  LAKES & RIVERS        -   \n",
       "1           1    400           no  LAKES & RIVERS        -   \n",
       "3           1    800           no  LAKES & RIVERS        -   \n",
       "4           1   1000           no  LAKES & RIVERS        -   \n",
       "5           1    200           no      INVENTIONS        -   \n",
       "...       ...    ...          ...             ...      ...   \n",
       "389438      2   1600           no            LOST        -   \n",
       "389440      2    400           no         FOUNDRY        -   \n",
       "389441      2    800           no         FOUNDRY        -   \n",
       "389442      2   1200           no         FOUNDRY        -   \n",
       "389443      2   1600           no         FOUNDRY        -   \n",
       "\n",
       "                                                   answer         question  \\\n",
       "0                 River mentioned most often in the Bible       the Jordan   \n",
       "1                                  Scottish word for lake             loch   \n",
       "3       American river only 33 miles shorter than the ...     the Missouri   \n",
       "4       World's largest lake, nearly 5 times as big as...  the Caspian Sea   \n",
       "5                            Marconi's wonderful wireless        the radio   \n",
       "...                                                   ...              ...   \n",
       "389438  In \"A Moveable Feast\", Gertrude Stein is quote...  Lost Generation   \n",
       "389440  This hefty noisemaker from Whitechapel Foundry...          Big Ben   \n",
       "389441  Around 4,000 years ago, the first foundries in...           bronze   \n",
       "389442  Several different foundries worked for 4 month...          Monitor   \n",
       "389443  Once one of the largest of its kind, the Gary ...       U.S. Steel   \n",
       "\n",
       "         air_date notes                                        q_a_and_cat  \n",
       "0      1984-09-10     -  River mentioned most often in the Bible the Jo...  \n",
       "1      1984-09-10     -         Scottish word for lake loch LAKES & RIVERS  \n",
       "3      1984-09-10     -  American river only 33 miles shorter than the ...  \n",
       "4      1984-09-10     -  World's largest lake, nearly 5 times as big as...  \n",
       "5      1984-09-10     -  Marconi's wonderful wireless the radio INVENTIONS  \n",
       "...           ...   ...                                                ...  \n",
       "389438 2021-08-13     -  In \"A Moveable Feast\", Gertrude Stein is quote...  \n",
       "389440 2021-08-13     -  This hefty noisemaker from Whitechapel Foundry...  \n",
       "389441 2021-08-13     -  Around 4,000 years ago, the first foundries in...  \n",
       "389442 2021-08-13     -  Several different foundries worked for 4 month...  \n",
       "389443 2021-08-13     -  Once one of the largest of its kind, the Gary ...  \n",
       "\n",
       "[363765 rows x 10 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['river', 'mentioned', 'most', 'often', 'in', 'the', 'bible', 'the', 'jordan', 'lakes', 'rivers']]\n"
     ]
    }
   ],
   "source": [
    "#tokenize text and remove punctuation\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "  for sentence in sentences:\n",
    "    yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "data_words = list(sent_to_words(df.q_a_and_cat))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['river', 'mentioned', 'most', 'often', 'in', 'the', 'bible', 'the', 'jordan', 'lakes_rivers']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-77dbc2e984b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Do lemmatization keeping only noun, adj, vb, adv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdata_lemmatized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_words_bigrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_postags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NOUN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ADJ'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'VERB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ADV'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lemmatized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-2628ab2648d7>\u001b[0m in \u001b[0;36mlemmatization\u001b[0;34m(texts, allowed_postags)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtexts_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mtexts_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_postags\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtexts_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__call__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Tagger.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Tagger.predict\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(seqs_in)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/thinc/api.py\u001b[0m in \u001b[0;36muniqued_fwd\u001b[0;34m(X, drop)\u001b[0m\n\u001b[1;32m    377\u001b[0m         )\n\u001b[1;32m    378\u001b[0m         \u001b[0mX_uniq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mY_uniq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbp_Y_uniq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_uniq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_uniq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mY_uniq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/thinc/neural/_classes/layernorm.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mbackprop_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_moments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mXhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/thinc/neural/_classes/layernorm.py\u001b[0m in \u001b[0;36m_get_moments\u001b[0;34m(ops, X)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mREPRODUCE_BUG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_get_moments_reproduce_bug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-08\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"f\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         ret = um.true_divide(\n\u001b[0m\u001b[1;32m    182\u001b[0m                 ret, rcount, out=ret, casting='unsafe', subok=False)\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_float16_result\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary \n",
    "id2word = corpora.Dictionary(data_lemmatized)  \n",
    "# Create Corpus \n",
    "texts = data_lemmatized  \n",
    "# Term Document Frequency \n",
    "corpus = [id2word.doc2bow(text) for text in texts]  \n",
    "# View \n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=40, \n",
    "                                           random_state=100,\n",
    "                                           update_every=5,\n",
    "                                           chunksize=1000,\n",
    "                                           passes=4,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the keyword of topics\n",
    "print(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el57231403253609150568725330432\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el57231403253609150568725330432_data = {\"mdsDat\": {\"x\": [0.23412403529356182, 0.04843910540706463, -0.08319486206998689, -0.08874497560945684, -0.11062330302118266], \"y\": [0.08796582342324455, -0.19939904151431576, 0.028523582312883163, 0.027815238755916117, 0.05509439702227177], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [36.13894751372534, 23.71263545891541, 15.092818612496437, 13.109905383117784, 11.945693031745035]}, \"tinfo\": {\"Term\": [\"great\", \"good\", \"black\", \"sea\", \"life\", \"man\", \"red\", \"green\", \"go\", \"hand\", \"horse\", \"little\", \"time\", \"lose\", \"call\", \"love\", \"night\", \"day\", \"dead\", \"fire\", \"state\", \"high\", \"walk\", \"blue\", \"ice\", \"old\", \"giant\", \"new\", \"home\", \"water\", \"man\", \"little\", \"day\", \"run\", \"bear\", \"girl\", \"back\", \"heart\", \"war\", \"light\", \"wine\", \"see\", \"break\", \"blood\", \"grace\", \"world\", \"dog\", \"ghost\", \"chicken\", \"tea\", \"work\", \"panama_canal\", \"music\", \"gin\", \"steel\", \"scale\", \"pirate\", \"field\", \"meet\", \"hat\", \"plane\", \"winter\", \"black\", \"go\", \"time\", \"tree\", \"eye\", \"friend\", \"sheep\", \"dollar\", \"foot\", \"school\", \"train\", \"white\", \"elephant\", \"wedding\", \"rain\", \"pie\", \"fall\", \"face\", \"long\", \"camel\", \"baseball\", \"take\", \"iceland\", \"gold\", \"beethoven\", \"chip\", \"tongue\", \"king\", \"deer\", \"corn\", \"red\", \"hand\", \"horse\", \"night\", \"dead\", \"walk\", \"water\", \"fly\", \"home\", \"hot\", \"death\", \"second\", \"order\", \"car\", \"doctor\", \"potato\", \"lobster\", \"cherry\", \"flag\", \"minute\", \"beast\", \"rise\", \"right\", \"drive\", \"rock\", \"spring\", \"planet\", \"beauty\", \"iron\", \"game\", \"short\", \"boat\", \"great\", \"green\", \"fire\", \"state\", \"blue\", \"boy\", \"dream\", \"fish\", \"bread\", \"summer\", \"coffee\", \"snake\", \"baby\", \"soldier\", \"oil\", \"golden\", \"empire\", \"mountain\", \"church\", \"independence\", \"wild\", \"double\", \"free\", \"play\", \"declaration\", \"age\", \"dance\", \"cheese\", \"american\", \"band\", \"tobacco\", \"good\", \"sea\", \"life\", \"lose\", \"love\", \"call\", \"high\", \"old\", \"ice\", \"new\", \"giant\", \"line\", \"space\", \"big\", \"last\", \"land\", \"live\", \"thing\", \"brother\", \"point\", \"arm\", \"egg\", \"first\", \"half\", \"sleep\", \"leave\", \"art\", \"picture\", \"show\", \"small\", \"put\", \"marble\"], \"Freq\": [665.0, 462.0, 654.0, 416.0, 392.0, 805.0, 423.0, 386.0, 530.0, 394.0, 389.0, 671.0, 458.0, 308.0, 280.0, 277.0, 307.0, 569.0, 301.0, 262.0, 260.0, 240.0, 269.0, 244.0, 233.0, 229.0, 223.0, 222.0, 247.0, 246.0, 804.8033330517496, 670.9558244474349, 568.8226770904728, 429.55085858651734, 358.5260586571634, 327.86068329263134, 337.2276864260714, 315.7756511648528, 283.07891767398445, 265.9202166149249, 251.15679295287578, 234.02874552109174, 232.01041888847644, 224.68664537470565, 211.24364768627888, 204.80868127834157, 202.07170972447486, 224.42154826474905, 205.7032020031076, 186.28786827426808, 189.51776595047474, 186.67997308628563, 173.00618468235712, 207.0940301279404, 182.03588685189973, 167.54522370662605, 170.22303524070261, 157.24966494341766, 159.57610742378438, 157.36192583203487, 182.64846578022326, 179.79808567778093, 653.3844405749309, 529.4563832264664, 457.5804322399212, 312.59101974020246, 283.366266788133, 281.16703620111974, 255.00775177143504, 235.61252407400087, 227.45728824588008, 221.95278260541116, 200.2654171040031, 194.48968781534566, 188.92893905748008, 187.76767172022772, 184.13209588289016, 190.16641719532802, 164.58948303963777, 164.81541097825004, 160.77131545204193, 162.070591383148, 157.91102985433758, 155.98436704990505, 153.58324099943553, 146.25547373638761, 143.97255054047554, 138.31024676124682, 140.25749263570847, 134.6966467115013, 141.79518238032253, 135.34153972070618, 422.64960174764974, 393.7203296249536, 388.8440374777651, 306.85174335480764, 301.20250921993545, 269.1565661241246, 245.2482834284898, 243.66403478039945, 246.494176690549, 225.3313079067339, 224.4593722820722, 219.61806393370745, 192.83636826502905, 188.29956533353302, 185.41780325116497, 184.7803023703006, 179.57023138589582, 168.1186342666152, 165.583556064518, 166.79304308690558, 157.1873928090969, 143.56455311850343, 138.75297172235076, 132.98775712066504, 130.79818773900786, 120.29955383500588, 121.93050595760141, 117.0873834817785, 106.71602711103408, 105.83710297264149, 106.24349844589591, 107.9257408993539, 664.7827096179677, 385.49248486363416, 261.4831184704554, 260.10308881853445, 243.83498741948517, 224.33627578503769, 176.6368118526369, 172.0982999488899, 174.05895270590682, 171.83698821496034, 156.71999786257228, 150.17597704519258, 150.12781711177223, 178.81479761278567, 143.5533281305276, 146.21258195186718, 137.7053298199645, 129.5121065976707, 131.7724539914938, 127.22745988068296, 124.46496527252977, 127.48852092152164, 120.039283166322, 124.91311498575219, 119.74673328260556, 116.37343391188101, 110.01746872594984, 102.8481477133519, 108.58423841881715, 94.2547556447626, 99.81725929145219, 461.6472457329076, 415.4379674152897, 391.75184201157833, 307.23544816111445, 277.13757131878884, 279.8722111595667, 239.86565402399862, 228.68577411646524, 232.59293158014373, 221.558659822605, 222.6745429891372, 171.84509392381844, 169.3704732984501, 166.20516031568334, 167.00032701019688, 163.53627583623205, 149.32610170895364, 148.2222111455418, 140.04635594615647, 138.44101140764542, 133.8609302189469, 122.36759466713437, 114.08693064306388, 104.3512212146907, 96.1122376051385, 88.15372590216775, 88.49494401475884, 89.22344978403672, 82.99967220154224, 142.4907095255544, 89.76395722249453, 117.40162903789648], \"Total\": [665.0, 462.0, 654.0, 416.0, 392.0, 805.0, 423.0, 386.0, 530.0, 394.0, 389.0, 671.0, 458.0, 308.0, 280.0, 277.0, 307.0, 569.0, 301.0, 262.0, 260.0, 240.0, 269.0, 244.0, 233.0, 229.0, 223.0, 222.0, 247.0, 246.0, 805.5610111128032, 671.7199918268979, 569.583834419726, 430.3166332170276, 359.2918451686547, 328.6248295868882, 338.02234381437336, 316.53292673409726, 283.83156774995524, 266.68779969856763, 251.9389942510696, 234.79890195281445, 232.79020547962077, 225.44904797065428, 212.0196985717359, 205.56522846588544, 202.82486627777556, 225.29182176927486, 206.5139668421436, 187.04963900167922, 190.3220790623288, 187.50340873982356, 173.77375464742687, 208.02919321595886, 182.89250428932178, 168.351491937003, 171.04258471589353, 158.02145589856846, 160.38180614081412, 158.1604229357278, 183.6304460414958, 186.26732030967975, 654.1470339423421, 530.2220387153606, 458.34255621589455, 313.3574473087181, 284.13364689835663, 281.94098378448734, 255.79520270046262, 236.40454234774958, 228.23607798422253, 222.73349551102808, 201.03936065231514, 195.28191690109577, 189.70078507434985, 188.55508833336054, 184.92701857735653, 191.0465784427771, 165.36216916653592, 165.59181465111254, 161.53834483885268, 162.8553024091532, 158.67565216294852, 156.75267132386736, 154.3553872708178, 147.0245021972801, 144.76304538075408, 139.0856125871182, 141.0469844889267, 135.45655260339166, 142.60363825376132, 136.1173420135879, 423.4327885480454, 394.5150862391958, 389.63390450739973, 307.6283623888155, 301.98022609962743, 269.94585660249504, 246.0243898879833, 244.4419334949186, 247.2893684047595, 226.1078717081327, 225.2362632137142, 220.4215101849185, 193.62649046844285, 189.08291332121138, 186.19763752651374, 185.57024296497556, 180.37409956367395, 168.90878644753596, 166.38022941362607, 167.5959785446988, 158.00645001551715, 144.33718391400296, 139.54166992735395, 133.76971784459187, 131.5760651826106, 121.07625994786994, 122.74479274673236, 117.87520084971118, 107.4954495428295, 106.61899405270356, 107.04928285654293, 108.87082662232875, 665.5644600004366, 386.2724921713521, 262.2631585085248, 260.9041363509967, 244.60951645237043, 225.11856577803175, 177.42322058042336, 172.88238338260294, 174.863177978526, 172.6439442247305, 157.5101250726608, 150.95740157389963, 150.9134847586272, 179.7710450577583, 144.34811913648537, 147.0419335594755, 138.51246839155334, 130.28835845534542, 132.56875471774964, 128.0155722172132, 125.23955338928472, 128.30988052838336, 120.81523171448036, 125.73456756681115, 120.53790146166402, 117.15113111557784, 110.81974736258358, 103.63240171941017, 109.45839751296232, 95.04580314044543, 100.98098651857332, 462.43775250941746, 416.2428968533907, 392.55398914057037, 308.0506370715578, 277.9255825161028, 280.6683952158825, 240.6566203613496, 229.46737932539824, 233.39485791965473, 222.3416274932654, 223.49123185994532, 172.63592485765412, 170.16617842209632, 166.99002173397713, 167.793170520565, 164.34092629117856, 150.11779501317676, 149.01046216751857, 140.835311692003, 139.2591349228639, 134.67667390509826, 123.17752603864895, 114.87929606525157, 105.1466960543588, 96.91211331931565, 88.94649819866945, 89.29737746724325, 90.07043736772681, 83.79300062014056, 143.8731888317281, 90.79753670018914, 121.97796153073118], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.1875, -4.3694, -4.5345, -4.8153, -4.9961, -5.0855, -5.0573, -5.123, -5.2324, -5.2949, -5.352, -5.4226, -5.4313, -5.4634, -5.5251, -5.556, -5.5695, -5.4646, -5.5516, -5.6508, -5.6336, -5.6487, -5.7248, -5.5449, -5.6739, -5.7568, -5.741, -5.8202, -5.8056, -5.8195, -5.6705, -5.6862, -3.9746, -4.1849, -4.3308, -4.7118, -4.81, -4.8178, -4.9154, -4.9945, -5.0298, -5.0543, -5.1571, -5.1863, -5.2153, -5.2215, -5.2411, -5.2088, -5.3533, -5.3519, -5.3767, -5.3687, -5.3947, -5.407, -5.4225, -5.4714, -5.4871, -5.5272, -5.5132, -5.5537, -5.5023, -5.5489, -3.9584, -4.0293, -4.0417, -4.2786, -4.2971, -4.4096, -4.5027, -4.5091, -4.4976, -4.5874, -4.5912, -4.613, -4.7431, -4.7669, -4.7823, -4.7858, -4.8144, -4.8803, -4.8955, -4.8882, -4.9475, -5.0381, -5.0722, -5.1147, -5.1313, -5.2149, -5.2015, -5.242, -5.3348, -5.343, -5.3392, -5.3235, -3.3646, -3.9096, -4.2977, -4.303, -4.3676, -4.4509, -4.69, -4.716, -4.7047, -4.7175, -4.8096, -4.8523, -4.8526, -4.6777, -4.8974, -4.879, -4.939, -5.0003, -4.983, -5.0181, -5.0401, -5.0161, -5.0763, -5.0365, -5.0787, -5.1073, -5.1634, -5.2308, -5.1766, -5.3181, -5.2607, -3.6363, -3.7417, -3.8005, -4.0435, -4.1466, -4.1367, -4.291, -4.3387, -4.3218, -4.3704, -4.3654, -4.6245, -4.639, -4.6579, -4.6531, -4.674, -4.7649, -4.7724, -4.8291, -4.8406, -4.8743, -4.9641, -5.0341, -5.1233, -5.2056, -5.292, -5.2881, -5.2799, -5.3522, -4.8118, -5.2739, -5.0055], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.0169, 1.0167, 1.0165, 1.016, 1.0157, 1.0155, 1.0154, 1.0154, 1.0151, 1.0149, 1.0147, 1.0145, 1.0144, 1.0144, 1.0141, 1.0141, 1.0141, 1.0139, 1.0139, 1.0137, 1.0136, 1.0134, 1.0134, 1.0133, 1.0131, 1.013, 1.013, 1.0129, 1.0128, 1.0127, 1.0124, 0.9825, 1.438, 1.4377, 1.4375, 1.4367, 1.4365, 1.4364, 1.4361, 1.4358, 1.4357, 1.4357, 1.4353, 1.4351, 1.4351, 1.435, 1.4349, 1.4345, 1.4345, 1.4345, 1.4344, 1.4343, 1.4343, 1.4342, 1.4341, 1.4339, 1.4337, 1.4336, 1.4335, 1.4335, 1.4335, 1.4334, 1.8891, 1.8889, 1.8889, 1.8884, 1.8884, 1.888, 1.8878, 1.8878, 1.8877, 1.8875, 1.8875, 1.8873, 1.8869, 1.8868, 1.8868, 1.8867, 1.8865, 1.8863, 1.8862, 1.8861, 1.8858, 1.8856, 1.8853, 1.8851, 1.885, 1.8845, 1.8843, 1.8842, 1.8837, 1.8836, 1.8834, 1.8822, 2.0306, 2.0298, 2.0288, 2.0287, 2.0286, 2.0283, 2.0274, 2.0273, 2.0272, 2.0271, 2.0268, 2.0266, 2.0266, 2.0265, 2.0263, 2.0261, 2.026, 2.0258, 2.0258, 2.0256, 2.0256, 2.0254, 2.0254, 2.0252, 2.0252, 2.0251, 2.0245, 2.0242, 2.0238, 2.0234, 2.0202, 2.1231, 2.1229, 2.1228, 2.1221, 2.122, 2.122, 2.1215, 2.1214, 2.1214, 2.1213, 2.1211, 2.1202, 2.1201, 2.1201, 2.1201, 2.1199, 2.1195, 2.1195, 2.1192, 2.1189, 2.1187, 2.1182, 2.1179, 2.1172, 2.1165, 2.1158, 2.1158, 2.1154, 2.1153, 2.1151, 2.1134, 2.0866]}, \"token.table\": {\"Topic\": [4, 4, 5, 5, 4, 1, 4, 2, 1, 3, 3, 2, 5, 2, 1, 4, 3, 4, 4, 1, 5, 5, 2, 3, 4, 3, 1, 2, 4, 4, 2, 4, 1, 3, 3, 4, 2, 3, 1, 2, 4, 4, 3, 5, 2, 4, 2, 2, 2, 1, 4, 5, 4, 3, 3, 2, 4, 2, 3, 1, 5, 1, 1, 2, 2, 4, 5, 1, 4, 4, 5, 3, 1, 1, 5, 3, 3, 3, 5, 2, 4, 3, 2, 5, 5, 5, 5, 1, 5, 1, 5, 3, 2, 5, 5, 1, 1, 2, 3, 4, 5, 1, 3, 4, 1, 5, 3, 4, 5, 3, 1, 5, 2, 1, 1, 3, 4, 5, 3, 5, 2, 3, 3, 3, 3, 1, 1, 2, 5, 3, 1, 2, 3, 5, 5, 5, 4, 4, 5, 3, 4, 1, 4, 2, 1, 5, 2, 4, 2, 2, 2, 3, 1, 3, 2, 2, 4, 1, 1, 2, 3, 4, 1, 1], \"Freq\": [0.99017396499192, 0.9958121302396371, 0.9949755671455394, 0.985471270220459, 0.993946963983449, 0.996975514095202, 0.9889968509298608, 0.9957419291886402, 0.9991877211448602, 0.9936303232214995, 0.9925751910206537, 0.9947290043619411, 0.9940713719077522, 0.9982465196923247, 0.9980082064009748, 0.9975082062987148, 0.9920012858417101, 0.9950312148881818, 0.9950636950071214, 0.9966055037496414, 0.9940688760370712, 0.9976185590281071, 0.9947480837498041, 0.9942728123753216, 0.9938976448589658, 0.9946196614951216, 0.997511224785409, 0.9921946449605763, 0.9957097377963565, 0.9967613188521978, 0.9917913324117338, 0.9926028764539454, 0.9989749807061838, 0.9967540056768351, 0.9945112603269342, 0.9955374910701005, 0.9957670206654394, 0.9935679230820359, 0.9959331107032711, 0.9982887708343839, 0.9897912731039166, 0.9976146268845824, 0.9942459485076728, 0.9904404149318644, 0.9963058398831867, 0.9963002002815756, 0.9960101631372008, 0.9964260633754184, 0.9978098426722307, 0.9935359670447277, 0.9951836219936179, 0.9923459135338701, 0.9948960480221392, 0.9977146959409413, 0.9981920716769008, 0.9945842129993665, 0.9932522439189871, 0.996662479601736, 0.9941943360261158, 0.9942660068211537, 0.9978020083568506, 0.9950526500629628, 0.9980986537515327, 0.9976952321364813, 0.9930317587751094, 0.992914037960103, 0.999053380683039, 0.995190547960378, 0.9991519078400968, 0.9967057137198689, 0.9890943215774847, 0.9986943813882861, 0.9926630005522976, 0.9983163624094471, 0.9972715466528048, 0.9947859933765972, 0.9983730766238088, 0.995100251487207, 0.9983081978618799, 0.9976976037111405, 0.9920668071889723, 0.9953909719440533, 0.9966295273678756, 0.9979254936742019, 0.9952729272704945, 0.9893587918823362, 0.9985887568184361, 0.9974209555167314, 0.9963163816674978, 0.9989281369682929, 0.9925538806836415, 0.9979259795914219, 0.9966673866852498, 0.9965894013998947, 0.9966696749981655, 0.999303577128158, 0.00819820226089005, 0.00819820226089005, 0.00819820226089005, 0.00819820226089005, 0.9591896645241359, 0.9976193924360791, 0.9964439567710759, 0.9977867673001326, 0.9955473445975961, 0.9984635018771923, 0.9979573977381796, 0.9975883361794537, 0.9979631992714072, 0.9967644382391728, 0.997315202197086, 0.9881155526828789, 0.9945218676444887, 0.9939045313327948, 0.9965667673575583, 0.9939321845752827, 0.9941577914409192, 0.9909583315768741, 0.996927077553683, 0.9912163178741007, 0.9949871112156131, 0.998977904971579, 0.9961182209755985, 0.9976639151127968, 0.9956218087095773, 0.9992641855029855, 0.9979121543090659, 0.9967068468559469, 0.997014010658713, 0.9980877084792456, 0.9965975055838422, 0.9968912524860999, 0.9901981327801226, 0.9905361949772454, 0.9905882423974149, 0.9869802786263467, 0.9936578030363689, 0.9957109608084518, 0.993146826044341, 0.9911108920251308, 0.996534603231509, 0.9951200608642227, 0.996270102449164, 0.9951983508956459, 0.9943884467926196, 0.9932188508590585, 0.9992526196591416, 0.990285433402922, 0.9925770515922735, 0.9948300638793184, 0.9988592985046691, 0.9964961247622043, 0.9970702069662392, 0.9958362262845172, 0.9970560946497548, 0.9934355575701102, 0.9901025406451922, 0.9962729300644352, 0.9663530870618636, 0.021474513045819193, 0.005368628261454798, 0.005368628261454798, 0.9983077157210786, 0.9972503692861692], \"Term\": [\"age\", \"american\", \"arm\", \"art\", \"baby\", \"back\", \"band\", \"baseball\", \"bear\", \"beast\", \"beauty\", \"beethoven\", \"big\", \"black\", \"blood\", \"blue\", \"boat\", \"boy\", \"bread\", \"break\", \"brother\", \"call\", \"camel\", \"car\", \"cheese\", \"cherry\", \"chicken\", \"chip\", \"church\", \"coffee\", \"corn\", \"dance\", \"day\", \"dead\", \"death\", \"declaration\", \"deer\", \"doctor\", \"dog\", \"dollar\", \"double\", \"dream\", \"drive\", \"egg\", \"elephant\", \"empire\", \"eye\", \"face\", \"fall\", \"field\", \"fire\", \"first\", \"fish\", \"flag\", \"fly\", \"foot\", \"free\", \"friend\", \"game\", \"ghost\", \"giant\", \"gin\", \"girl\", \"go\", \"gold\", \"golden\", \"good\", \"grace\", \"great\", \"green\", \"half\", \"hand\", \"hat\", \"heart\", \"high\", \"home\", \"horse\", \"hot\", \"ice\", \"iceland\", \"independence\", \"iron\", \"king\", \"land\", \"last\", \"leave\", \"life\", \"light\", \"line\", \"little\", \"live\", \"lobster\", \"long\", \"lose\", \"love\", \"man\", \"marble\", \"marble\", \"marble\", \"marble\", \"marble\", \"meet\", \"minute\", \"mountain\", \"music\", \"new\", \"night\", \"oil\", \"old\", \"order\", \"panama_canal\", \"picture\", \"pie\", \"pirate\", \"plane\", \"planet\", \"play\", \"point\", \"potato\", \"put\", \"rain\", \"red\", \"right\", \"rise\", \"rock\", \"run\", \"scale\", \"school\", \"sea\", \"second\", \"see\", \"sheep\", \"short\", \"show\", \"sleep\", \"small\", \"snake\", \"soldier\", \"space\", \"spring\", \"state\", \"steel\", \"summer\", \"take\", \"tea\", \"thing\", \"time\", \"tobacco\", \"tongue\", \"train\", \"tree\", \"walk\", \"war\", \"water\", \"wedding\", \"white\", \"wild\", \"wine\", \"winter\", \"winter\", \"winter\", \"winter\", \"work\", \"world\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 4, 3, 5, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el57231403253609150568725330432\", ldavis_el57231403253609150568725330432_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el57231403253609150568725330432\", ldavis_el57231403253609150568725330432_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el57231403253609150568725330432\", ldavis_el57231403253609150568725330432_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0      0.234124  0.087966       1        1  36.138948\n",
       "3      0.048439 -0.199399       2        1  23.712635\n",
       "2     -0.083195  0.028524       3        1  15.092819\n",
       "4     -0.088745  0.027815       4        1  13.109905\n",
       "1     -0.110623  0.055094       5        1  11.945693, topic_info=         Term        Freq       Total Category  logprob  loglift\n",
       "105     great  665.000000  665.000000  Default  30.0000  30.0000\n",
       "74       good  462.000000  462.000000  Default  29.0000  29.0000\n",
       "316     black  654.000000  654.000000  Default  28.0000  28.0000\n",
       "248       sea  416.000000  416.000000  Default  27.0000  27.0000\n",
       "247      life  392.000000  392.000000  Default  26.0000  26.0000\n",
       "...       ...         ...         ...      ...      ...      ...\n",
       "851   picture   89.223450   90.070437   Topic5  -5.2799   2.1154\n",
       "852      show   82.999672   83.793001   Topic5  -5.3522   2.1153\n",
       "1356    small  142.490710  143.873189   Topic5  -4.8118   2.1151\n",
       "888       put   89.763957   90.797537   Topic5  -5.2739   2.1134\n",
       "859    marble  117.401629  121.977962   Topic5  -5.0055   2.0866\n",
       "\n",
       "[187 rows x 6 columns], token_table=      Topic      Freq      Term\n",
       "term                           \n",
       "468       4  0.990174       age\n",
       "1528      4  0.995812  american\n",
       "2440      5  0.994976       arm\n",
       "987       5  0.985471       art\n",
       "80        4  0.993947      baby\n",
       "...     ...       ...       ...\n",
       "1918      2  0.021475    winter\n",
       "1918      3  0.005369    winter\n",
       "1918      4  0.005369    winter\n",
       "471       1  0.998308      work\n",
       "545       1  0.997250     world\n",
       "\n",
       "[164 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 4, 3, 5, 2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyldavis\n",
      "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from pyldavis) (0.24.2)\n",
      "Requirement already satisfied: jinja2 in /Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from pyldavis) (2.11.2)\n",
      "Requirement already satisfied: joblib in /Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from pyldavis) (0.17.0)\n",
      "Requirement already satisfied: future in /Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from pyldavis) (0.18.2)\n",
      "Requirement already satisfied: gensim in /Users/sallypants/.local/lib/python3.8/site-packages (from pyldavis) (4.1.2)\n",
      "Collecting numpy>=1.20.0\n",
      "  Downloading numpy-1.22.2-cp38-cp38-macosx_10_14_x86_64.whl (17.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.6 MB 5.4 MB/s eta 0:00:01    |▋                               | 337 kB 2.0 MB/s eta 0:00:09     |███▎                            | 1.8 MB 2.0 MB/s eta 0:00:08     |███▊                            | 2.0 MB 2.0 MB/s eta 0:00:08     |████████████████████▏           | 11.1 MB 239 kB/s eta 0:00:28\n",
      "\u001b[?25hCollecting pandas>=1.2.0\n",
      "  Downloading pandas-1.4.1-cp38-cp38-macosx_10_9_x86_64.whl (11.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.4 MB 1.6 MB/s eta 0:00:01    |████████▌                       | 3.0 MB 1.5 MB/s eta 0:00:06     |███████████▉                    | 4.2 MB 3.9 MB/s eta 0:00:02     |█████████████▌                  | 4.8 MB 3.9 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: sklearn in /Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from pyldavis) (0.0)\n",
      "Collecting numexpr\n",
      "  Downloading numexpr-2.8.1-cp38-cp38-macosx_10_9_x86_64.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting funcy\n",
      "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: scipy in /Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from pyldavis) (1.5.2)\n",
      "Requirement already satisfied: setuptools in /Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from pyldavis) (50.3.0.post20201103)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from scikit-learn->pyldavis) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from jinja2->pyldavis) (1.1.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from gensim->pyldavis) (3.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from pandas>=1.2.0->pyldavis) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from pandas>=1.2.0->pyldavis) (2020.1)\n",
      "Requirement already satisfied: packaging in /Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from numexpr->pyldavis) (20.4)\n",
      "Requirement already satisfied: requests in /Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from smart-open>=1.8.1->gensim->pyldavis) (2.24.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->pyldavis) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from packaging->numexpr->pyldavis) (2.4.7)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim->pyldavis) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim->pyldavis) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim->pyldavis) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim->pyldavis) (2.10)\n",
      "Building wheels for collected packages: pyldavis\n",
      "  Building wheel for pyldavis (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyldavis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136900 sha256=37acf0890a3107127c8a46b4a32b670e5002c9bf32bb8fcf5cd78fd44cebc850\n",
      "  Stored in directory: /Users/sallypants/Library/Caches/pip/wheels/90/61/ec/9dbe9efc3acf9c4e37ba70fbbcc3f3a0ebd121060aa593181a\n",
      "Successfully built pyldavis\n",
      "Installing collected packages: numpy, pandas, numexpr, funcy, pyldavis\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.5\n",
      "    Uninstalling numpy-1.18.5:\n",
      "      Successfully uninstalled numpy-1.18.5\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.1.3\n",
      "    Uninstalling pandas-1.1.3:\n",
      "      Successfully uninstalled pandas-1.1.3\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "tensorflow 2.3.1 requires numpy<1.19.0,>=1.16.0, but you'll have numpy 1.22.2 which is incompatible.\u001b[0m\n",
      "Successfully installed funcy-1.17 numexpr-2.8.1 numpy-1.22.2 pandas-1.4.1 pyldavis-3.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
