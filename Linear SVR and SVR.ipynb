{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVR and SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, Ridge\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"combined_season1-37.tsv.zip\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round</th>\n",
       "      <th>value</th>\n",
       "      <th>daily_double</th>\n",
       "      <th>category</th>\n",
       "      <th>comments</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>air_date</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>no</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>River mentioned most often in the Bible</td>\n",
       "      <td>the Jordan</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>no</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>Scottish word for lake</td>\n",
       "      <td>loch</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>yes</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>River in this famous song:</td>\n",
       "      <td>the Volga River</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>no</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>American river only 33 miles shorter than the ...</td>\n",
       "      <td>the Missouri</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>no</td>\n",
       "      <td>LAKES &amp; RIVERS</td>\n",
       "      <td>-</td>\n",
       "      <td>World's largest lake, nearly 5 times as big as...</td>\n",
       "      <td>the Caspian Sea</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389440</th>\n",
       "      <td>2</td>\n",
       "      <td>400</td>\n",
       "      <td>no</td>\n",
       "      <td>FOUNDRY</td>\n",
       "      <td>-</td>\n",
       "      <td>This hefty noisemaker from Whitechapel Foundry...</td>\n",
       "      <td>Big Ben</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389441</th>\n",
       "      <td>2</td>\n",
       "      <td>800</td>\n",
       "      <td>no</td>\n",
       "      <td>FOUNDRY</td>\n",
       "      <td>-</td>\n",
       "      <td>Around 4,000 years ago, the first foundries in...</td>\n",
       "      <td>bronze</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389442</th>\n",
       "      <td>2</td>\n",
       "      <td>1200</td>\n",
       "      <td>no</td>\n",
       "      <td>FOUNDRY</td>\n",
       "      <td>-</td>\n",
       "      <td>Several different foundries worked for 4 month...</td>\n",
       "      <td>Monitor</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389443</th>\n",
       "      <td>2</td>\n",
       "      <td>1600</td>\n",
       "      <td>no</td>\n",
       "      <td>FOUNDRY</td>\n",
       "      <td>-</td>\n",
       "      <td>Once one of the largest of its kind, the Gary ...</td>\n",
       "      <td>U.S. Steel</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389444</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>19th CENTURY AMERICAN WOMEN</td>\n",
       "      <td>-</td>\n",
       "      <td>2 of the 3 women depicted on the first statue ...</td>\n",
       "      <td>(2 of) (Sojourner) Truth, (Susan B.) Anthony, ...</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>389445 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        round  value daily_double                     category comments  \\\n",
       "0           1    100           no               LAKES & RIVERS        -   \n",
       "1           1    200           no               LAKES & RIVERS        -   \n",
       "2           1    800          yes               LAKES & RIVERS        -   \n",
       "3           1    400           no               LAKES & RIVERS        -   \n",
       "4           1    500           no               LAKES & RIVERS        -   \n",
       "...       ...    ...          ...                          ...      ...   \n",
       "389440      2    400           no                      FOUNDRY        -   \n",
       "389441      2    800           no                      FOUNDRY        -   \n",
       "389442      2   1200           no                      FOUNDRY        -   \n",
       "389443      2   1600           no                      FOUNDRY        -   \n",
       "389444      3      0           no  19th CENTURY AMERICAN WOMEN        -   \n",
       "\n",
       "                                                   answer  \\\n",
       "0                 River mentioned most often in the Bible   \n",
       "1                                  Scottish word for lake   \n",
       "2                              River in this famous song:   \n",
       "3       American river only 33 miles shorter than the ...   \n",
       "4       World's largest lake, nearly 5 times as big as...   \n",
       "...                                                   ...   \n",
       "389440  This hefty noisemaker from Whitechapel Foundry...   \n",
       "389441  Around 4,000 years ago, the first foundries in...   \n",
       "389442  Several different foundries worked for 4 month...   \n",
       "389443  Once one of the largest of its kind, the Gary ...   \n",
       "389444  2 of the 3 women depicted on the first statue ...   \n",
       "\n",
       "                                                 question    air_date notes  \n",
       "0                                              the Jordan  1984-09-10     -  \n",
       "1                                                    loch  1984-09-10     -  \n",
       "2                                         the Volga River  1984-09-10     -  \n",
       "3                                            the Missouri  1984-09-10     -  \n",
       "4                                         the Caspian Sea  1984-09-10     -  \n",
       "...                                                   ...         ...   ...  \n",
       "389440                                            Big Ben  2021-08-13     -  \n",
       "389441                                             bronze  2021-08-13     -  \n",
       "389442                                            Monitor  2021-08-13     -  \n",
       "389443                                         U.S. Steel  2021-08-13     -  \n",
       "389444  (2 of) (Sojourner) Truth, (Susan B.) Anthony, ...  2021-08-13     -  \n",
       "\n",
       "[389445 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['air_date'] = pd.to_datetime(df['air_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On 11/26/2001, the values for the questions doubled for both rounds of Jeopardy. Need to adjust the earlier episodes to have the same values as post-11/26/2001 shows.\n",
    "df.loc[df['air_date'] < '2001-11-26', \"value\"] = df.value * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove Daily Doubles since the contestants can wager any amounts for those\n",
    "df = df[df[\"daily_double\"] != 'yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only standard values (this will remove Final Jeopardy questions, which do not have a set amount and set are at '0', as well as the handful of non-standard values that are likely typos)\n",
    "df = df.loc[df['value'].isin([200, 400, 600, 800, 1000, 400, 800, 1200, 1600, 2000])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import string to remove punctuation\n",
    "\n",
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_punctuation(text):\n",
    "    nopunct=[words for words in text if words not in string.punctuation]\n",
    "    words_without_punct=''.join(nopunct)\n",
    "    return words_without_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation and lowercase words in 'category,' 'answer,' and 'question'\n",
    "\n",
    "df['category'] = df['category'].apply(lambda x: no_punctuation(x).lower())\n",
    "df['answer'] = df['answer'].apply(lambda x: no_punctuation(x).lower())\n",
    "df['question'] = df['question'].apply(lambda x: no_punctuation(x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove numerals from 'category,' 'answer,' and 'question'\n",
    "\n",
    "df['category'] = df['category'].str.replace('\\d+', '')\n",
    "df['answer'] = df['answer'].str.replace('\\d+', '')\n",
    "df['question'] = df['question'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round</th>\n",
       "      <th>value</th>\n",
       "      <th>daily_double</th>\n",
       "      <th>category</th>\n",
       "      <th>comments</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>air_date</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>no</td>\n",
       "      <td>lakes  rivers</td>\n",
       "      <td>-</td>\n",
       "      <td>river mentioned most often in the bible</td>\n",
       "      <td>the jordan</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>no</td>\n",
       "      <td>lakes  rivers</td>\n",
       "      <td>-</td>\n",
       "      <td>scottish word for lake</td>\n",
       "      <td>loch</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>no</td>\n",
       "      <td>lakes  rivers</td>\n",
       "      <td>-</td>\n",
       "      <td>american river only  miles shorter than the mi...</td>\n",
       "      <td>the missouri</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>no</td>\n",
       "      <td>lakes  rivers</td>\n",
       "      <td>-</td>\n",
       "      <td>worlds largest lake nearly  times as big as su...</td>\n",
       "      <td>the caspian sea</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>no</td>\n",
       "      <td>inventions</td>\n",
       "      <td>-</td>\n",
       "      <td>marconis wonderful wireless</td>\n",
       "      <td>the radio</td>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389438</th>\n",
       "      <td>2</td>\n",
       "      <td>1600</td>\n",
       "      <td>no</td>\n",
       "      <td>lost</td>\n",
       "      <td>-</td>\n",
       "      <td>in a moveable feast gertrude stein is quoted a...</td>\n",
       "      <td>lost generation</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389440</th>\n",
       "      <td>2</td>\n",
       "      <td>400</td>\n",
       "      <td>no</td>\n",
       "      <td>foundry</td>\n",
       "      <td>-</td>\n",
       "      <td>this hefty noisemaker from whitechapel foundry...</td>\n",
       "      <td>big ben</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389441</th>\n",
       "      <td>2</td>\n",
       "      <td>800</td>\n",
       "      <td>no</td>\n",
       "      <td>foundry</td>\n",
       "      <td>-</td>\n",
       "      <td>around  years ago the first foundries in mesop...</td>\n",
       "      <td>bronze</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389442</th>\n",
       "      <td>2</td>\n",
       "      <td>1200</td>\n",
       "      <td>no</td>\n",
       "      <td>foundry</td>\n",
       "      <td>-</td>\n",
       "      <td>several different foundries worked for  months...</td>\n",
       "      <td>monitor</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389443</th>\n",
       "      <td>2</td>\n",
       "      <td>1600</td>\n",
       "      <td>no</td>\n",
       "      <td>foundry</td>\n",
       "      <td>-</td>\n",
       "      <td>once one of the largest of its kind the gary w...</td>\n",
       "      <td>us steel</td>\n",
       "      <td>2021-08-13</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>363765 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        round  value daily_double       category comments  \\\n",
       "0           1    200           no  lakes  rivers        -   \n",
       "1           1    400           no  lakes  rivers        -   \n",
       "3           1    800           no  lakes  rivers        -   \n",
       "4           1   1000           no  lakes  rivers        -   \n",
       "5           1    200           no     inventions        -   \n",
       "...       ...    ...          ...            ...      ...   \n",
       "389438      2   1600           no           lost        -   \n",
       "389440      2    400           no        foundry        -   \n",
       "389441      2    800           no        foundry        -   \n",
       "389442      2   1200           no        foundry        -   \n",
       "389443      2   1600           no        foundry        -   \n",
       "\n",
       "                                                   answer         question  \\\n",
       "0                 river mentioned most often in the bible       the jordan   \n",
       "1                                  scottish word for lake             loch   \n",
       "3       american river only  miles shorter than the mi...     the missouri   \n",
       "4       worlds largest lake nearly  times as big as su...  the caspian sea   \n",
       "5                             marconis wonderful wireless        the radio   \n",
       "...                                                   ...              ...   \n",
       "389438  in a moveable feast gertrude stein is quoted a...  lost generation   \n",
       "389440  this hefty noisemaker from whitechapel foundry...          big ben   \n",
       "389441  around  years ago the first foundries in mesop...           bronze   \n",
       "389442  several different foundries worked for  months...          monitor   \n",
       "389443  once one of the largest of its kind the gary w...         us steel   \n",
       "\n",
       "         air_date notes  \n",
       "0      1984-09-10     -  \n",
       "1      1984-09-10     -  \n",
       "3      1984-09-10     -  \n",
       "4      1984-09-10     -  \n",
       "5      1984-09-10     -  \n",
       "...           ...   ...  \n",
       "389438 2021-08-13     -  \n",
       "389440 2021-08-13     -  \n",
       "389441 2021-08-13     -  \n",
       "389442 2021-08-13     -  \n",
       "389443 2021-08-13     -  \n",
       "\n",
       "[363765 rows x 9 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use 'answer' (Jeopardy questions) as the feature and 'value' as the target\n",
    "\n",
    "X = df['answer']\n",
    "y = df['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25, random_state=54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the vector with stop words, max_features=1000, and bigrams\n",
    "tfidf = TfidfVectorizer(stop_words='english', ngram_range= (1, 2), max_features=1000)\n",
    "\n",
    "# Fit the vectorizer on X_train and transform it\n",
    "X_train_vectorized = tfidf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_vectorized: (272823, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train_vectorized:\", X_train_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-538.015546  , -538.15476162])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Basic Linear SVR\n",
    "\n",
    "linear_SVR_regressor = LinearSVR()\n",
    "\n",
    "linear_SVR_cv = cross_val_score(linear_SVR_regressor, X_train_vectorized, y_train, scoring='neg_root_mean_squared_error', cv=2)\n",
    "linear_SVR_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Linear SVR: 538.0851538096358\n"
     ]
    }
   ],
   "source": [
    "print(\"Basic Linear SVR:\", -(linear_SVR_cv.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-527.24034472, -527.60050114])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tweaking Linear SVR\n",
    "\n",
    "linear_SVR_regressor_l2loss = LinearSVR(loss='squared_epsilon_insensitive')\n",
    "\n",
    "linear_SVR_cv_l2loss = cross_val_score(linear_SVR_regressor_l2loss, X_train_vectorized, y_train, scoring='neg_root_mean_squared_error', cv=2)\n",
    "linear_SVR_cv_l2loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Linear SVR: 538.0851538096358\n",
      "Linear SVR L2 Loss: 527.4204229282802\n"
     ]
    }
   ],
   "source": [
    "print(\"Basic Linear SVR:\", -(linear_SVR_cv.mean()))\n",
    "print(\"Linear SVR L2 Loss:\", -(linear_SVR_cv_l2loss.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-527.24054804, -527.60054926])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tweaking Linear SVR\n",
    "\n",
    "linear_SVR_regressor_tol = LinearSVR(loss='squared_epsilon_insensitive', tol = 1e-8)\n",
    "\n",
    "linear_SVR_cv_tol = cross_val_score(linear_SVR_regressor_tol, X_train_vectorized, y_train, scoring='neg_root_mean_squared_error', cv=2)\n",
    "linear_SVR_cv_tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Linear SVR: 538.0851538096358\n",
      "Linear SVR L2 Loss: 527.4204229282802\n",
      "Linear SVR Tolerance: 527.4205486470335\n"
     ]
    }
   ],
   "source": [
    "print(\"Basic Linear SVR:\", -(linear_SVR_cv.mean()))\n",
    "print(\"Linear SVR L2 Loss:\", -(linear_SVR_cv_l2loss.mean()))\n",
    "print(\"Linear SVR Tolerance:\", -(linear_SVR_cv_tol.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-527.06857972, -527.45422942])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tweaking Linear SVR\n",
    "\n",
    "linear_SVR_regressor_c = LinearSVR(loss='squared_epsilon_insensitive', C=.1)\n",
    "\n",
    "linear_SVR_cv_c = cross_val_score(linear_SVR_regressor_c, X_train_vectorized, y_train, scoring='neg_root_mean_squared_error', cv=2)\n",
    "linear_SVR_cv_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Linear SVR: 538.0851538096358\n",
      "Linear SVR L2 Loss: 527.4204229282802\n",
      "Linear SVR Tolerance: 527.4205486470335\n",
      "Linear SVR C: 527.2614045728969\n"
     ]
    }
   ],
   "source": [
    "print(\"Basic Linear SVR:\", -(linear_SVR_cv.mean()))\n",
    "print(\"Linear SVR L2 Loss:\", -(linear_SVR_cv_l2loss.mean()))\n",
    "print(\"Linear SVR Tolerance:\", -(linear_SVR_cv_tol.mean()))\n",
    "print(\"Linear SVR C:\", -(linear_SVR_cv_c.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-526.16061581, -527.81008967, -524.38984161, -528.41854293,\n",
       "       -527.20302308])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tweaking Linear SVR\n",
    "\n",
    "linear_SVR_regressor_verbose = LinearSVR(loss='squared_epsilon_insensitive', C=.1, verbose = 10)\n",
    "\n",
    "linear_SVR_cv_verbose = cross_val_score(linear_SVR_regressor_verbose, X_train_vectorized, y_train, scoring='neg_root_mean_squared_error', cv=5)\n",
    "linear_SVR_cv_verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Linear SVR: 538.0851538096358\n",
      "Linear SVR L2 Loss: 527.4204229282802\n",
      "Linear SVR Tolerance: 527.4205486470335\n",
      "Linear SVR Tolerance: 527.2614045728969\n",
      "Linear SVR Verbose: 526.7964226200212\n"
     ]
    }
   ],
   "source": [
    "print(\"Basic Linear SVR:\", -(linear_SVR_cv.mean()))\n",
    "print(\"Linear SVR L2 Loss:\", -(linear_SVR_cv_l2loss.mean()))\n",
    "print(\"Linear SVR Tolerance:\", -(linear_SVR_cv_tol.mean()))\n",
    "print(\"Linear SVR Tolerance:\", -(linear_SVR_cv_c.mean()))\n",
    "print(\"Linear SVR Verbose:\", -(linear_SVR_cv_verbose.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run test data with tuned linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=0.1, loss='squared_epsilon_insensitive', verbose=10)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross validate best tuned Linear SVC\n",
    "\n",
    "# Train the model using the training sets\n",
    "linear_SVR_regressor_verbose.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize X_test\n",
    "X_test_vectorized = tfidf.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the testing set\n",
    "y_pred = linear_SVR_regressor_verbose.predict(X_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_best_linear_svc = mean_squared_error(y_test, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "535.7827738378692"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_best_linear_svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:246: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "/Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:246: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-578.34792741, -579.24311364])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Basic SVR\n",
    "\n",
    "SVR_regressor = SVR(max_iter=100)\n",
    "\n",
    "SVR_cv = cross_val_score(SVR_regressor, X_train_vectorized, y_train, scoring='neg_root_mean_squared_error', cv=2)\n",
    "SVR_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic SVR: 578.7955205264072\n"
     ]
    }
   ],
   "source": [
    "print(\"Basic SVR:\", -(SVR_cv.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:246: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "/Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:246: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-576.35614576, -577.24640192])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tweaking SVR\n",
    "\n",
    "SVR_regressor_max = SVR(max_iter=5000)\n",
    "\n",
    "SVR_cv_max = cross_val_score(SVR_regressor_max, X_train_vectorized, y_train, scoring='neg_root_mean_squared_error', cv=2)\n",
    "SVR_cv_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic SVR: 578.7955205264072\n",
      "SVR increased max_iter: 576.8012738368016\n"
     ]
    }
   ],
   "source": [
    "print(\"Basic SVR:\", -(SVR_cv.mean()))\n",
    "print(\"SVR increased max_iter:\", -(SVR_cv_max.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:246: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "/Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:246: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-577.6139814 , -578.40234812])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tweaking SVR\n",
    "\n",
    "SVR_regressor_linearkernel = SVR(max_iter=5000, kernel='linear')\n",
    "\n",
    "SVR_cv_linearkernel = cross_val_score(SVR_regressor_linearkernel, X_train_vectorized, y_train, scoring='neg_root_mean_squared_error', cv=2)\n",
    "SVR_cv_linearkernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic SVR: 578.7955205264072\n",
      "SVR increased max_iter: 576.8012738368016\n",
      "SVR linear kernel: 578.0081647598281\n"
     ]
    }
   ],
   "source": [
    "print(\"Basic SVR:\", -(SVR_cv.mean()))\n",
    "print(\"SVR increased max_iter:\", -(SVR_cv_max.mean()))\n",
    "print(\"SVR linear kernel:\", -(SVR_cv_linearkernel.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:246: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "/Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:246: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-577.95419264, -578.85632656])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tweaking SVR\n",
    "\n",
    "SVR_regressor_polykernel = SVR(max_iter=5000, kernel='poly')\n",
    "\n",
    "SVR_cv_polykernel = cross_val_score(SVR_regressor_polykernel, X_train_vectorized, y_train, scoring='neg_root_mean_squared_error', cv=2)\n",
    "SVR_cv_polykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic SVR: 578.7955205264072\n",
      "SVR increased max_iter: 576.8012738368016\n",
      "SVR linear kernel: 578.0081647598281\n",
      "SVR poly kernel: 578.0081647598281\n"
     ]
    }
   ],
   "source": [
    "print(\"Basic SVR:\", -(SVR_cv.mean()))\n",
    "print(\"SVR increased max_iter:\", -(SVR_cv_max.mean()))\n",
    "print(\"SVR linear kernel:\", -(SVR_cv_linearkernel.mean()))\n",
    "print(\"SVR poly kernel:\", -(SVR_cv_linearkernel.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:246: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "/Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:246: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-577.95419264, -578.85632656])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tweaking SVR\n",
    "\n",
    "SVR_regressor_sigmoidkernel = SVR(max_iter=5000, kernel='poly')\n",
    "\n",
    "SVR_cv_sigmoidkernel = cross_val_score(SVR_regressor_sigmoidkernel, X_train_vectorized, y_train, scoring='neg_root_mean_squared_error', cv=2)\n",
    "SVR_cv_sigmoidkernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic SVR: 578.7955205264072\n",
      "SVR increased max_iter: 576.8012738368016\n",
      "SVR linear kernel: 578.0081647598281\n",
      "SVR poly kernel: 578.0081647598281\n",
      "SVR sigmoid kernel: 578.4052595989834\n"
     ]
    }
   ],
   "source": [
    "print(\"Basic SVR:\", -(SVR_cv.mean()))\n",
    "print(\"SVR increased max_iter:\", -(SVR_cv_max.mean()))\n",
    "print(\"SVR linear kernel:\", -(SVR_cv_linearkernel.mean()))\n",
    "print(\"SVR poly kernel:\", -(SVR_cv_linearkernel.mean()))\n",
    "print(\"SVR sigmoid kernel:\", -(SVR_cv_sigmoidkernel.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will stick with the default kernel (rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:246: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "/Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:246: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-578.3286645 , -579.21267189])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tweaking SVR\n",
    "\n",
    "SVR_regressor_gamma = SVR(max_iter=5000, gamma='auto')\n",
    "\n",
    "SVR_cv_gamma = cross_val_score(SVR_regressor_gamma, X_train_vectorized, y_train, scoring='neg_root_mean_squared_error', cv=2)\n",
    "SVR_cv_gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic SVR: 578.7955205264072\n",
      "SVR increased max_iter: 576.8012738368016\n",
      "SVR linear kernel: 578.0081647598281\n",
      "SVR poly kernel: 578.0081647598281\n",
      "SVR sigmoid kernel: 578.4052595989834\n",
      "SVR gamma: 578.7706681965676\n"
     ]
    }
   ],
   "source": [
    "print(\"Basic SVR:\", -(SVR_cv.mean()))\n",
    "print(\"SVR increased max_iter:\", -(SVR_cv_max.mean()))\n",
    "print(\"SVR linear kernel:\", -(SVR_cv_linearkernel.mean()))\n",
    "print(\"SVR poly kernel:\", -(SVR_cv_linearkernel.mean()))\n",
    "print(\"SVR sigmoid kernel:\", -(SVR_cv_sigmoidkernel.mean()))\n",
    "print(\"SVR gamma:\", -(SVR_cv_gamma.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will stick with the default gamma (scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:246: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "/Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:246: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-576.35614576, -577.24640192])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tweaking SVR\n",
    "\n",
    "SVR_regressor_tol = SVR(max_iter=5000, tol=1e-8)\n",
    "\n",
    "SVR_cv_tol = cross_val_score(SVR_regressor_tol, X_train_vectorized, y_train, scoring='neg_root_mean_squared_error', cv=2)\n",
    "SVR_cv_tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic SVR: 578.7955205264072\n",
      "SVR increased max_iter: 576.8012738368016\n",
      "SVR linear kernel: 578.0081647598281\n",
      "SVR poly kernel: 578.0081647598281\n",
      "SVR sigmoid kernel: 578.4052595989834\n",
      "SVR gamma: 578.7706681965676\n",
      "SVR tolerance for stopping criterion: 578.7706681965676\n"
     ]
    }
   ],
   "source": [
    "print(\"Basic SVR:\", -(SVR_cv.mean()))\n",
    "print(\"SVR increased max_iter:\", -(SVR_cv_max.mean()))\n",
    "print(\"SVR linear kernel:\", -(SVR_cv_linearkernel.mean()))\n",
    "print(\"SVR poly kernel:\", -(SVR_cv_linearkernel.mean()))\n",
    "print(\"SVR sigmoid kernel:\", -(SVR_cv_sigmoidkernel.mean()))\n",
    "print(\"SVR gamma:\", -(SVR_cv_gamma.mean()))\n",
    "print(\"SVR tolerance for stopping criterion:\", -(SVR_cv_gamma.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will keep default tolerance (0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:246: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "/Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:246: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-576.6325775 , -577.63279238])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tweaking SVR\n",
    "\n",
    "SVR_regressor_c = SVR(max_iter=5000, C=0.8)\n",
    "\n",
    "SVR_cv_c = cross_val_score(SVR_regressor_c, X_train_vectorized, y_train, scoring='neg_root_mean_squared_error', cv=2)\n",
    "SVR_cv_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic SVR: 578.7955205264072\n",
      "SVR increased max_iter: 576.8012738368016\n",
      "SVR linear kernel: 578.0081647598281\n",
      "SVR poly kernel: 578.0081647598281\n",
      "SVR sigmoid kernel: 578.4052595989834\n",
      "SVR gamma: 578.7706681965676\n",
      "SVR tolerance for stopping criterion: 578.7706681965676\n",
      "SVR smaller C: 577.1326849432296\n"
     ]
    }
   ],
   "source": [
    "print(\"Basic SVR:\", -(SVR_cv.mean()))\n",
    "print(\"SVR increased max_iter:\", -(SVR_cv_max.mean()))\n",
    "print(\"SVR linear kernel:\", -(SVR_cv_linearkernel.mean()))\n",
    "print(\"SVR poly kernel:\", -(SVR_cv_linearkernel.mean()))\n",
    "print(\"SVR sigmoid kernel:\", -(SVR_cv_sigmoidkernel.mean()))\n",
    "print(\"SVR gamma:\", -(SVR_cv_gamma.mean()))\n",
    "print(\"SVR tolerance for stopping criterion:\", -(SVR_cv_gamma.mean()))\n",
    "print(\"SVR smaller C:\", -(SVR_cv_c.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:246: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "/Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:246: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "/Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:246: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "/Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:246: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "/Users/sallypants/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:246: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-575.27553439, -576.48081035, -576.15486822, -577.0988126 ,\n",
       "       -575.99102385])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross Validate best tuned SVR\n",
    "\n",
    "SVR_regressor_max = SVR(max_iter=10000)\n",
    "\n",
    "SVR_regressor_cv_max = cross_val_score(SVR_regressor_max, X_train_vectorized, y_train, scoring='neg_root_mean_squared_error', cv=5)\n",
    "SVR_regressor_cv_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR increased max_iter: 576.2002098825078\n"
     ]
    }
   ],
   "source": [
    "print(\"SVR increased max_iter:\", -(SVR_regressor_cv_max.mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
